{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUi37sVKvG1Z"
   },
   "source": [
    "# 521153S Deep Learning assignment 3: Training CNN networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaS0RL3xvG1b"
   },
   "source": [
    "## Outline\n",
    "#### In this assignment, you will learn:\n",
    "* Using a custom Pytorch dataset, and Splitting it to train, validation, and test subsets.\n",
    "* Building a Simple Convolutional Neural Network (CNN) in Pytorch.\n",
    "* Training and testing your CNN with Pytorch.\n",
    "\n",
    "#### Tasks (<span style=\"color:green\">10 points</span>)\n",
    "* **Part 1.** Initializing the CIFAR-10 dataset and splitting it (<span style=\"color:green\">3 points</span>)\n",
    "    * 1.1. Initializing the dataset and splitting it (<span style=\"color:green\">1 point</span>)\n",
    "    * 1.2. Samples checking (<span style=\"color:green\">1 point</span>)\n",
    "    * 1.3. Iterating through the dataset with batches (<span style=\"color:green\">1 point</span>)<br>\n",
    "* **Part 2.** Build a CNN on your own. (<span style=\"color:green\">3 points</span>) <br>\n",
    "* **Part 3.** Train and test your CNN. (<span style=\"color:green\">4 points</span>)\n",
    "    * 3.1. Create functions for train and evaluation (<span style=\"color:green\">2.5 points</span>) <br>\n",
    "    * 3.2. Train and test your network (<span style=\"color:green\">1.5 points</span>) <br>\n",
    "\n",
    "#### Environment\n",
    "Python 3, Numpy, matplotlib, torch, torchvision\n",
    "\n",
    "#### Dataset\n",
    "* [**CIFAR-10**](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "is a classic benchmark dataset in machine learning, especially for image classification tasks. It’s widely used to train and evaluate models that can recognize objects in images. It consists of 60,000 colored RGB images, each sized 32×32 pixels, divided into 10 distinct classes.\n",
    "\n",
    "The dataset is initially divided into 50,000 training samples and 10,000 test samples. In this assignment, you'll retain the default test split and further partition the original training set into 45,000 samples for training and 5,000 for validation.\n",
    "\n",
    "#### Hints\n",
    "* To find the place where you have to insert your solution, hit Crtl + F and search for **TODO:** . You are not supposed to modify the codes from other parts.\n",
    "* Be careful with the shapes of the tensors flowing through the CNN model, making all the operations have compatible inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYk6guTDvG1c"
   },
   "source": [
    "## Part 1. Initializing the CIFAR-10 dataset and splitting it (<span style=\"color:green\">3 points</span>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XqExv9fvG1d"
   },
   "source": [
    "### Part 1.1. Initialize and Split the Dataset (<span style=\"color:green\">1 point</span>)\n",
    "Here we initialize and download the default CIFAR-10 training and test splits using the [CIFAR10](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html) class then splitting the default training split into training and validation sets.\n",
    "\n",
    "#### Note\n",
    "One of the most important input parameters when initializing any dataset in pytorch is `transform`. It is often used for data augmentation and converting the images to tensor format, further illustrations of `transform` can be found in [https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#writing-custom-datasets-dataloaders-and-transforms](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#writing-custom-datasets-dataloaders-and-transforms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPyXopd4ZDtF"
   },
   "outputs": [],
   "source": [
    "! pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJE2TbYOvG1f"
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j89iE45O6byf"
   },
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# TODO: Initialize and download the full CIFAR-10 training split, set transform to transform (0.5 point)\n",
    "# Hint: use datasets.CIFAR10()\n",
    "# your code here\n",
    "# full_trainset =  #\n",
    "\n",
    "# TODO: Initialize and download the CIFAR-10 test split, set transform to transform (0.5 point)\n",
    "# Hint: use datasets.CIFAR10()\n",
    "# your code here\n",
    "# test_set =  #\n",
    "\n",
    "# Splitting the full training set into training and validation splits\n",
    "# Total number of samples\n",
    "total_samples = len(full_trainset)  # 50,000\n",
    "\n",
    "# Calculate split indices\n",
    "train_end = int(0.9 * total_samples)  # 45,000\n",
    "val_start = train_end                 # 45,000 to 50,000\n",
    "\n",
    "# Create subsets\n",
    "train_set = Subset(full_trainset, range(0, train_end))\n",
    "val_set = Subset(full_trainset, range(val_start, total_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HgwPm2gvG1g"
   },
   "source": [
    "### Part 1.2. Samples Checking (<span style=\"color:green\">1 point</span>)\n",
    "First, randomly show some images in training, validation and testing data and check its corresponding labels.<br>\n",
    "Secondly, check the number of images for each class in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AyBIV7tvG1g"
   },
   "outputs": [],
   "source": [
    "# checking training set\n",
    "# randomly show some samples and their labels\n",
    "\n",
    "# Define Classes order as indicated in the dataset description\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "num_to_show = 5\n",
    "idx = np.random.choice(range(len(train_set)), num_to_show, replace=False) # randomly pick 5 pictures to show\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "for i in range(len(idx)):\n",
    "    image, label = train_set[idx[i]]\n",
    "    label_name = classes[label]\n",
    "\n",
    "    ax = plt.subplot(1, num_to_show, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('class #{}'.format(label_name))\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Convert tensor to numpy image\n",
    "    img = image / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print number of images for each class\n",
    "print('total number of training set: {}'.format(len(train_set)))\n",
    "train_targets = [full_trainset.targets[idx] for idx in train_set.indices]\n",
    "for i in range(10):\n",
    "    print('numer of images for class {}: {}'.format(classes[i], np.sum(np.array(train_targets) == i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dm1E_imtvG1g"
   },
   "outputs": [],
   "source": [
    "# TODO:checking validation data (0.5 points)\n",
    "# Hint:like the training set:show 5 images from validation set and  print number of images for each class\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsXIxYXovG1h"
   },
   "outputs": [],
   "source": [
    "# TODO: checking testing data (0.5 points)\n",
    "# Hint:like the training set:show 5 images from test set and  print number of images for each class\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn9fIbJmvG1h"
   },
   "source": [
    "### Part 1.3. Iterating through the dataset with batches (<span style=\"color:green\">1 point</span>)\n",
    "PyTorch’s DataLoader class(`torch.utils.data.DataLoader` ) helps feed data into a model during training efficiently . It automatically handles mini-batching, so we don’t have to slice data manually. It supports shuffling, which improves generalization, and parallel data loading, which prevents the GPU from waiting on slow I/O. It also neatly pairs inputs and labels as tensors, simplifying the training loop. Here, we add more features with the help of [DataLoader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#iterating-through-the-dataset):\n",
    "* Batching the data\n",
    "* Shuffling the data\n",
    "* Load the data in parallel using multiprocessing workers.\n",
    "\n",
    "`torch.utils.data.DataLoader` is an iterator which provides all these features. Parameters used below should be clear.\n",
    "\n",
    "The `DataLoader` takes as input an instance of any `Dataset` class like `CIFAR10` that we used in section 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hV4e-5dNvG1h"
   },
   "outputs": [],
   "source": [
    "# Set batch_size to 64, shuffling the training set. Number of workers here is set to 0. If your system is Linux,\n",
    "# it is possible to try more workers to do multi-process parallel reading.\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "# TODO: create test_loader and valid_loader, both with no shuffling (1 points)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5a-9V1NvG1h"
   },
   "source": [
    "## Part 2. Build a CNN network on your own (<span style=\"color:green\">3 points</span>)\n",
    "A Convolutional Neural Network (CNN) is built from a sequence of building blocks, each designed to progressively learn spatial and semantic features from images. A typical CNN architecture is composed of the following building blocks: Input image → [Convolutional layer + ReLU + Pooling layer] (repeated multiple times) → Flattening → Fully Connected layer(s) → Output layer (with Softmax activation for classification). In PyTorch, the building blocks of a CNN can be defined using the following functions: nn.Conv2d() for convolutional layers, nn.BatchNorm2d() for batch normalization layers, nn.ReLU() for non-linear activation functions, nn.MaxPool2d() for pooling layers, and nn.Linear() for fully connected layers. These layers are typically declared and initialized in the __init__() function of a custom nn.Module class, and then connected sequentially in the forward() function to define the flow of data through the network. We can define a SimpleCNN class as follows："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwyCwkoQcZq8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # building blocks defined here\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(16 * 16 * 16, num_classes)  # assuming input 3×32×32\n",
    "\n",
    "    def forward(self, x):\n",
    "        # connect the blocks sequentially\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.fc(x)  # fully connected layer\n",
    "        return x\n",
    "\n",
    "# example usage\n",
    "model = SimpleCNN(num_classes=10)\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f0sWSiovG1h"
   },
   "source": [
    "In this assignment, you need to define your own Network shown in the figure below, following the rule of thumb:\n",
    "1. Define the layers in `__init__`.\n",
    "2. Do the forward calculation in `forward`.\n",
    "(**Conv**: convolutional layer, **BN**: Batch Normalization layer, **Max_pool**: max pooling layer, **FC**: fully connected layer ):\n",
    "<!--![model.png](attachment:model.png)-->\n",
    "<img src=https://i.postimg.cc/kMQVG7zN/Assignment-3-CNN-Arch.png width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxKWSs-NvG1i"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # TODO: define layers (1.5 points)\n",
    "        # Hint:use nn.Conv2d() for convolutional layer, nn.BatchNorm2d() for Batch Normalization layer,nn.MaxPool2d() for max pooling layer, nn.Linear() for fully connected layer,nn.ReLU() for relu\n",
    "        # your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: forward pass (1.5 points)\n",
    "        # your code here\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkSb4fI5dQpM"
   },
   "outputs": [],
   "source": [
    "model = Network(num_classes=10)\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4WXs7dOvG1i"
   },
   "source": [
    "## Part 3. Train and test your CNN model (<span style=\"color:green\">4 points</span>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8sU_FxXedWS"
   },
   "source": [
    "### Part 3.1. Create functions for train and evaluation (<span style=\"color:green\">2.5 points</span>)\n",
    "* Instantiate a network.\n",
    "* When training, create an optimizer to take care of network's parameters, calculate the loss and accuracy.\n",
    "* When evaluating, only the accuracy needs to be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "461cB7FAvG1i"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# instantiate a network\n",
    "net = Network(num_classes=10).to(device)\n",
    "\n",
    "# evaluation function\n",
    "@torch.no_grad()\n",
    "def eval(net, data_loader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    num_images = 0\n",
    "    for i_batch, (images, labels) in enumerate(data_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outs = net(images)\n",
    "        preds = outs.argmax(dim=1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        num_images += labels.size(0)\n",
    "\n",
    "    acc = correct / num_images if num_images > 0 else 0.0\n",
    "    return acc\n",
    "\n",
    "# training function\n",
    "def train(net, train_loader, valid_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # TODO: build your SGD optimizer with learning rate=0.01, momentum=0.9, no weight decay (0.5 points)\n",
    "    # your code here\n",
    "    # optimizer =\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    epochs = 10\n",
    "    best_acc_eval = 0.0\n",
    "    best_model_state = None\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        correct = 0 # used to accumulate number of correctly recognized images\n",
    "        num_images = 0 # used to accumulate number of images\n",
    "        for i_batch, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # TODO: rest of the training code\n",
    "            # your code here, including the forward propagation (0.75 points),\n",
    "            # backward propagation (0.75 points) and calculating the accuracy by counting correct and num_images (0.5 points)\n",
    "\n",
    "        acc = correct / num_images if num_images > 0 else 0.0\n",
    "        acc_eval = eval(net, valid_loader)\n",
    "\n",
    "        # Save best model\n",
    "        if acc_eval > best_acc_eval:\n",
    "          best_acc_eval = acc_eval\n",
    "          best_model_state = net.state_dict()\n",
    "\n",
    "        print('epoch: %d, lr: %f, accuracy: %f, loss: %f, valid accuracy: %f' % (epoch, optimizer.param_groups[0]['lr'], acc, loss.item(), acc_eval))\n",
    "\n",
    "    net.load_state_dict(best_model_state)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3z0EUJEvG1i"
   },
   "source": [
    "### Part 3.2. Train and test your network (<span style=\"color:green\">1.5 points</span>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qj2r3os2vG1i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell will run fastly if you use GPU\n",
    "\n",
    "print('Please wait patiently, it may take some seconds...')\n",
    "# TODO: train your network here (0.75 points)\n",
    "# Hint: Use the train function you implemented previously\n",
    "# your code here\n",
    "\n",
    "# TODO: test your network here on testing data (0.75 points)\n",
    "# Hint: Use the eval function you implemented previously\n",
    "# your code here\n",
    "\n",
    "print('accuracy on testing data: %f' % acc_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
